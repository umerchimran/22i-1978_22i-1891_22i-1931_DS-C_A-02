{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvHlwtcAYl9M"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "from multiprocessing import Pool\n",
        "\n",
        "class Doc_Ind:\n",
        "    def _init_(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def extracts(self, line):\n",
        "        article_id, section_text = line.split(\",\", 1)\n",
        "        words = re.findall(r'\\w+', section_text.lower())\n",
        "        return [(word, article_id) for word in words]\n",
        "\n",
        "    def count_documents(self, word_article_pairs):\n",
        "\n",
        "        word_count = defaultdict(set)\n",
        "\n",
        "        for word, article_id in word_article_pairs:\n",
        "\n",
        "            word_count[word].add(article_id)\n",
        "\n",
        "        return [(word, len(articles)) for word, articles in word_count.items()]\n",
        "\n",
        "    def run(self):\n",
        "        with Pool() as pool:\n",
        "            word_article_pairs = sum(pool.map(self.extracts, self.data), [])\n",
        "            word_counts = self.count_documents(word_article_pairs)\n",
        "        return word_counts\n",
        "\n",
        "class Indexer:\n",
        "    def _init_(self, total_documents):\n",
        "        self.total_documents = total_documents\n",
        "\n",
        "    def compute(self, args):\n",
        "        word, document_count, current_article_id, section_text = args\n",
        "        word = word.strip('\"')\n",
        "        document_count = int(document_count)\n",
        "        idf = math.log10(self.total_documents / document_count)\n",
        "\n",
        "        words = re.findall(r'\\w+', section_text.lower())\n",
        "        tf = words.count(word) / len(words)\n",
        "        tfidf = tf * idf\n",
        "        return word, (current_article_id, tfidf)\n",
        "\n",
        "    def combine(self, tfidf_values):\n",
        "        tfidf_dict = {}\n",
        "        for word, tfidf in tfidf_values:\n",
        "            tfidf_dict[word] = tfidf\n",
        "        return tfidf_dict\n",
        "\n",
        "    def run(self, word_document_count, current_article):\n",
        "\n",
        "        current_article_id, section_text = current_article.split(\",\", 1)\n",
        "\n",
        "        tfidf_values = [self.compute_tfidf((word, count, current_article_id, section_text)) for word, count in word_document_count]\n",
        "\n",
        "        return self.combine_tfidf(tfidf_values)\n",
        "\n",
        "class Query:\n",
        "\n",
        "    def _init_(self, query, total_documents):\n",
        "\n",
        "        self.query = query\n",
        "\n",
        "        self.total_documents = total_documents\n",
        "\n",
        "\n",
        "    def calculate_relevance(self, tfidf_values):\n",
        "        query_vector = {word: idf for word, idf in tfidf_values}\n",
        "        dot_product = sum(query_vector.get(word, 0) * tfidf for word, tfidf in tfidf_values.values())\n",
        "        return None, dot_product\n",
        "\n",
        "    def rank_documents(self, relevance_scores):\n",
        "        return sorted(relevance_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    def run(self, tfidf_values):\n",
        "        relevance_scores = self.calculate_relevance(tfidf_values)\n",
        "        return self.rank_documents(relevance_scores)\n",
        "\n",
        "class QueryRee:\n",
        "    def _init_(self, data, relevant_documents):\n",
        "        self.data = data\n",
        "        self.relevant_documents = relevant_documents\n",
        "\n",
        "    def fetch_(self, line):\n",
        "        article_id, section_text = line.split(\",\", 1)\n",
        "        if article_id in self.relevant_documents:\n",
        "            return article_id, section_text\n",
        "\n",
        "    def run(self):\n",
        "        relevant_documents = filter(None, map(self.fetch_, self.data))\n",
        "        return relevant_documents\n",
        "\n",
        "if _name_ == '_main_':\n",
        "\n",
        "    test_da = [\n",
        "        \"1,\\\"Introduction: MapReduce is a programming model and an associated implementation for processing and generating large datasets. It was developed at Google to support distributed computing on large clusters of commodity hardware. The model is inspired by the map and reduce functions commonly used in functional programming, although their purpose in the MapReduce framework is not the same as in their original forms.\\\"\",\n",
        "        \"2,\\\"Map Function: In the MapReduce framework, the map function takes input key/value pairs and processes each pair to generate a set of intermediate key/value pairs. The output of the map function is then shuffled and sorted to group together pairs with the same intermediate key, which are then passed to the reduce function.\\\"\",\n",
        "        \"3,\\\"Reduce Function: The reduce function receives a key and a set of associated values, and it processes these values to produce a final result. The output of the reduce function is typically written to a file system or another storage system. The MapReduce framework automatically handles many of the details of distributed computing, such as fault tolerance, data distribution, and load balancing.\\\"\",\n",
        "        \"4,\\\"Usage: MapReduce is commonly used for processing large-scale datasets in distributed environments. It is particularly well-suited for tasks such as data processing, log analysis, web indexing, and machine learning. Many popular distributed computing frameworks, such as Apache Hadoop and Apache Spark, are based on the MapReduce model.\\\"\",\n",
        "        \"5,\\\"Example: As an example, consider a word count program implemented using MapReduce. The program would read input text and emit key/value pairs where the key is a word and the value is the number 1. The reduce function would then receive these key/value pairs and sum up the values for each output word, providing a total count of each word in the input text.\\\"\"\n",
        "    ]\n",
        "\n",
        "    # Document indexing\n",
        "    dor = Doc_Ind(test_data)\n",
        "\n",
        "    document_count = dor.run()\n",
        "\n",
        "\n",
        "    indexer = Indexer(len(test_data))\n",
        "\n",
        "    tfidf_values = []\n",
        "\n",
        "    for article in test_data:\n",
        "\n",
        "        tfidf_values.append(indexer.run(document_count, article))\n",
        "\n",
        "    print(\"Document Count:\", document_count)\n",
        "\n",
        "    for i, tfidf in enumerate(tfidf_values, start=1):\n",
        "\n",
        "        print(f\"TF-IDF for Document {i}:\", tfidf)\n",
        "\n",
        "    # Query processing\n",
        "    query = \"map function\"\n",
        "    query_processor = QueryProcessor(query, len(test_data))\n",
        "    relevance_scores = query_processor.run(tfidf_values)\n",
        "    print(\"Relevance Scores:\", relevance_scores)\n",
        "\n",
        "    # Query response\n",
        "    relevant_documents = ['3', '7', '5']\n",
        "\n",
        "    query_response = QueryResponse(test_data, relevant_documents)\n",
        "\n",
        "    relevant_documents = query_response.run()\n",
        "\n",
        "    print(\"Output:\", list(relevant_documents))"
      ]
    }
  ]
}