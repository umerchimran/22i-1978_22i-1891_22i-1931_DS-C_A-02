{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvHlwtcAYl9M"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "class IndexingMapper:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def extract_words(self, line):\n",
        "        article_id, section_text = line\n",
        "        words = re.findall(r'\\w+', section_text.lower())\n",
        "        return [(word, article_id) for word in words]\n",
        "\n",
        "    def map(self, line):\n",
        "        return self.extract_words(line)\n",
        "\n",
        "class IndexerMapper:\n",
        "    def __init__(self, total_documents):\n",
        "        self.total_documents = total_documents\n",
        "\n",
        "    def compute_tfidf(self, args):\n",
        "\n",
        "        word, document_count, current_article_id, section_text = args\n",
        "\n",
        "        word = word.strip('\"')\n",
        "        document_count = int(document_count)\n",
        "\n",
        "        idf = math.log10(self.total_documents / document_count)\n",
        "\n",
        "        words = re.findall(r'\\w+', section_text.lower())\n",
        "        tf = words.count(word) / len(words)\n",
        "\n",
        "        tfidf = tf * idf\n",
        "\n",
        "        return word, (current_article_id, tfidf)\n",
        "\n",
        "    def map(self, word_document_count, current_article):\n",
        "\n",
        "        current_article_id, section_text = current_article\n",
        "\n",
        "        tfidf_values = [self.compute_tfidf((word, count, current_article_id, section_text)) for word, count in word_document_count]\n",
        "\n",
        "        return tfidf_values\n",
        "\n",
        "class QueryrMapper:\n",
        "    def __init__(self, query, total_documents):\n",
        "\n",
        "        self.query = query\n",
        "\n",
        "        self.total_documents = total_documents\n",
        "\n",
        "    def vectorize_query(self):\n",
        "\n",
        "        words = re.findall(r'\\w+', self.query.lower())\n",
        "        return [(word, 1) for word in set(words)]\n",
        "\n",
        "    def map(self, tfidf_values):\n",
        "        relevance_scores = self.calculate_rele(tfidf_values)\n",
        "        return relevance_scores\n",
        "\n",
        "    def calculate_rele(self, tfidf_values):\n",
        "\n",
        "        query_vector = {word: idf for word, idf in tfidf_values}\n",
        "\n",
        "        dot_prod = sum(query_vector.get(word, 0) * tfidf for word, tfidf in tfidf_values.values())\n",
        "\n",
        "        return None, dot_prod\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    data = []\n",
        "    with open('wikipedia_data.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
        "\n",
        "        reader = csv.reader(csvfile)\n",
        "\n",
        "        next(reader)\n",
        "\n",
        "        for row in reader:\n",
        "\n",
        "            data.append((row[0], row[3]))\n",
        "\n",
        "\n",
        "    mapper =IndexMapper()\n",
        "\n",
        "    word_article_pairs = sum(map(mapper.extract_words, data), [])\n",
        "\n",
        "    mapper = IndexerMapper(len(data))\n",
        "\n",
        "    tfidf_values = []\n",
        "\n",
        "    for article in data:\n",
        "\n",
        "        tfidf_values.append(mapper.map(document_count, article))\n",
        "\n",
        "    query = \"map function\"\n",
        "\n",
        "    mapper = QueryMapper(query, len(data))\n",
        "\n",
        "    relevance_scores = mapper.map(tfidf_values)\n",
        "\n",
        "    relevant_documents = ['3', '7',s '5']\n",
        "\n",
        "    for doc_id in relevant_documents:\n",
        "\n",
        "        print(doc_id)\n"
      ]
    }
  ]
}